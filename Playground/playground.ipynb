{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Beat Detector Comparison\n",
    "## Important Variables\n",
    "Important variables used throughout the code can be changed here.\n",
    "- `segmentation_window_size`: every signal in a database is sliced into pieces of `segmentation_window_size` seconds to make the signals more comparable\n",
    "- `tolerance_window_size`: to determine wether a QRS complex was determined correctly a tolerance window of Â±`tolerance_window_size` milliseconds is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_window_size = 10 # in seconds, default is 10\n",
    "tolerance_window_size = 150 # in milliseconds, default is 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Start Engines\n",
    "Install all important libraries with pip apart from `matlab.engine` which has to be installed from the MATLAB root folder. You can find more information on instlaling the `matlab.engine` [here](https://de.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "import numpy as np\n",
    "from cProfile import label\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "import pathlib\n",
    "from ecgdetectors import Detectors\n",
    "import sys\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "import pandas\n",
    "import os\n",
    "from detectors.visgraphdetector import VisGraphDetector\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "current_working_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the `matlab.engine` to be able to execute MATLAB commands or functions from within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Signal\n",
    "Some initial parameters for the generation of the synthetic signals. Here it is possible to choose diffent types of artificial noise and also choose whether the signal should be real or synthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Initial parameters---\n",
    "rrLength = 50       # A desired ECG signal length (the number of RR intervals) \n",
    "APBrate = 0.10      # Rate of atrial premature beats (APB). A number between 0 and 0.5\n",
    "onlyRR = 0          # 1 - only RR intervals are generated, 0 - multilead ECG is generated\n",
    "\n",
    "medEpis = 15        # Median duration of an atrial fibrillation (AF) episode\n",
    "stayInAF = float(1-np.log(2)/medEpis)   # Probability to stay in AF state\n",
    "AFburden = 0.8      # AF burden. 0 - the entire signal is sinus rhythm (SR), 1 - the entire signal is AF\n",
    "\n",
    "noiseType = 4       # Type of noise. A number from 0 to 4. 0 - no noise added (noise RMS = 0 mV), \n",
    "                    # 1 - motion artefacts, 2 - electrode movement artefacts, 3 - baseline wander, \n",
    "                    # 4 - mixture of type 1, type 2 and type 3 noises\n",
    "noiseRMS = 0.02     # Noise level in milivolts \n",
    "\n",
    "realRRon = 1       # 1 - real RR series are used, 0 - synthetic\n",
    "realVAon = 1       # 1 - real ventricular activity is used, 0 - synthetic\n",
    "realAAon = 1       # 1 - real atrial activity is used, 0 - synthetic\n",
    "# Note: cannot select real atrial activity and synthetic ventricular activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next field the signal is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_generator_path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Signal_generator')\n",
    "eng.addpath(signal_generator_path, nargout=0)\n",
    "\n",
    "returndata = eng.simPAF_ECG_generator(rrLength, realRRon, realVAon, realAAon, AFburden, stayInAF, APBrate, noiseType, noiseRMS, onlyRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classes and Functions\n",
    "Function to determine the peaks in a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_peaks(segment, detector):\n",
    "\n",
    "    if isinstance(segment, RecordingSegment):\n",
    "        signal = segment.Signal\n",
    "        fs = segment.Fs\n",
    "\n",
    "    elif isinstance(segment, Recording):\n",
    "        signal = segment.WholeSignal\n",
    "        fs = segment.Fs\n",
    "\n",
    "    else:\n",
    "        raise Exception('You have to input either a RecordingSegment or a Recording as the segment.')\n",
    "        \n",
    "    \n",
    "    if detector.short_name == \"match_fil\" and (fs != 250 and fs != 360):\n",
    "        print(detector.short_name, \"could not run because the sample rate is wrong and was skipped\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        predicted_peaks = detector.predicted_qrs_compelx(signal=signal, fs=fs)\n",
    "    except IndexError:\n",
    "        print(detector.short_name, \"failed due to an index error and was skipped\")\n",
    "        return []\n",
    "    \n",
    "    return predicted_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification(segment, predicted_peaks):\n",
    "\n",
    "    if isinstance(segment, RecordingSegment):\n",
    "        signal = segment.Signal\n",
    "        fs = segment.Fs\n",
    "        actual_peaks = segment.Actual_Qrs_Complex\n",
    "\n",
    "    elif isinstance(segment, Recording):\n",
    "        signal = segment.WholeSignal\n",
    "        fs = segment.Fs\n",
    "        actual_peaks = segment.WholeActual_Qrs_Complex\n",
    "\n",
    "    else:\n",
    "        raise Exception('You have to input either a RecordingSegment or a Recording as the segment.')\n",
    "\n",
    "    pp = len(predicted_peaks)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    actual_peaks_iter = actual_peaks\n",
    "\n",
    "    for predicted_peak in predicted_peaks:\n",
    "        tpdetect = 1\n",
    "        for i in range(len(actual_peaks_iter)):\n",
    "            if predicted_peak >= (actual_peaks_iter[i] - tolerance_window_size) and predicted_peak <= (actual_peaks_iter[i] + tolerance_window_size):\n",
    "                tp+=1\n",
    "                tpdetect = 0\n",
    "                actual_peaks_iter = np.delete(actual_peaks_iter, i)\n",
    "                break\n",
    "        if tpdetect:\n",
    "            fp+=1\n",
    "\n",
    "    for actualpeak in actual_peaks_iter:\n",
    "        fn+=1\n",
    "\n",
    "    return pp, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calculation(tp, fp, fn):\n",
    "    try:\n",
    "        sensitivity = tp / (tp+fn)\n",
    "    except ZeroDivisionError:\n",
    "        sensitivity = 0\n",
    "    try:\n",
    "        positive_predictivity = tp / (tp+fp)\n",
    "    except ZeroDivisionError:\n",
    "        positive_predictivity = 0\n",
    "    try:    \n",
    "        f1_score = tp / (tp+.5*(fp+fn))\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 0\n",
    "    return sensitivity, positive_predictivity, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary classes for the detectors and the databases are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector():\n",
    "    def __init__(self, name, short_name, algorithm) -> None:\n",
    "        self.name = name\n",
    "        self.algorithm = algorithm\n",
    "        self.short_name = short_name\n",
    "    \n",
    "    def predicted_qrs_compelx(self, signal, fs):\n",
    "        return self.algorithm(signal, fs)\n",
    "\n",
    "    def name(self):\n",
    "        return self.name\n",
    "\n",
    "    def short_name(self):\n",
    "        return self.short_name\n",
    "\n",
    "class Database():\n",
    "    def __init__(self, Name, Users, Fs) -> None:\n",
    "        self.Name = Name\n",
    "        self.Users = Users\n",
    "        self.Fs = Fs\n",
    "\n",
    "    def Name(self):\n",
    "        return self.Name\n",
    "\n",
    "    def Users(self):\n",
    "        return self.Users\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, RecordingSegment, Detector, predict_peaks, binary_classification, score_calculation) -> None:\n",
    "        self.RecordingSegment = RecordingSegment\n",
    "        self.Detector = Detector\n",
    "        self.predict_peaks = predict_peaks\n",
    "        self.binary_classification = binary_classification\n",
    "        self.score_calculation = score_calculation\n",
    "        self.predicted_peaks = None\n",
    "        self.pp = None\n",
    "        self.tp = None\n",
    "        self.fp = None\n",
    "        self.fn = None\n",
    "        self.sensitivity = None\n",
    "        self.positive_predictivity = None\n",
    "        self.f1_score = None\n",
    "    \n",
    "    def calculate(self):\n",
    "        self.predicted_peaks = self.predict_peaks(self.RecordingSegment,self.Detector)\n",
    "        self.pp, self.tp, self.fp, self.fn = self.binary_classification(self.RecordingSegment, self.predicted_peaks)\n",
    "        self.sensitivity, self.positive_predictivity, self.f1_score = self.score_calculation(self.tp, self.fp, self.fn)\n",
    "        return self\n",
    "\n",
    "class RecordingSegment():\n",
    "    def __init__(self, Signal, Actual_Qrs_Complex, Fs) -> None:\n",
    "        self.Signal = Signal\n",
    "        self.Actual_Qrs_Complex = Actual_Qrs_Complex\n",
    "        self.Fs = Fs\n",
    "        self.Evaluations = []\n",
    "\n",
    "    def Signal(self):\n",
    "        return self.Signal\n",
    "\n",
    "    def Actual_Qrs_Complex(self):\n",
    "        return self.Actual_Qrs_Complex\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs\n",
    "\n",
    "    def Evaluation(self, detectors, predict_peaks, binary_classification, score_calculation):\n",
    "        evaluations = []\n",
    "        for detector in detectors:\n",
    "            evaluations.append(Evaluation(self,detector,predict_peaks, binary_classification, score_calculation).calculate())\n",
    "        return evaluations\n",
    "\n",
    "class Recording():\n",
    "    def __init__(self, RecordingName, RecordingSegments, WholeSignal, WholeActual_Qrs_Complex, Fs) -> None:\n",
    "        self.RecordingName = RecordingName\n",
    "        self.RecordingSegments = RecordingSegments\n",
    "        self.WholeSignal = WholeSignal\n",
    "        self.WholeActual_Qrs_Complex = WholeActual_Qrs_Complex\n",
    "        self.Fs = Fs\n",
    "\n",
    "    def RecordingName(self):\n",
    "        return self.RecordingName\n",
    "    \n",
    "    def RecordingSegments(self):\n",
    "        return self.RecordingSegments\n",
    "\n",
    "    def WholeSignal(self):\n",
    "        return self.WholeSignal\n",
    "    \n",
    "    def WholeActual_Qrs_Complex(self):\n",
    "        return self.WholeActual_Qrs_Complex\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs\n",
    "\n",
    "class User():\n",
    "    def __init__(self, UserName,Recordings) -> None:\n",
    "        self.UserName = UserName\n",
    "        self.Recordings = Recordings\n",
    "\n",
    "    def UserName(self):\n",
    "        return self.UserName\n",
    "\n",
    "    def Recordings(self):\n",
    "        return self.Recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to split signals in smaller parts including the respecitve qrs complexes. `split_signal` returns an array of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_signal(signal, fs, actual_qrs_complexes):\n",
    "    signal = np.array(signal)\n",
    "    actual_qrs_complexes = np.array(actual_qrs_complexes)\n",
    "    split = [[] for i in range(((len(signal)//(fs*segmentation_window_size)*2)-1))]\n",
    "\n",
    "    for split_signal_counter in range((len(signal)//(fs*segmentation_window_size)*2)-1):\n",
    "        min_index = split_signal_counter*(fs*segmentation_window_size//2)\n",
    "        max_index = split_signal_counter*(fs*segmentation_window_size//2)+fs*segmentation_window_size-1\n",
    "\n",
    "        split[split_signal_counter].append(signal[min_index:max_index])\n",
    "\n",
    "        min_counter = 0\n",
    "        max_counter = 0\n",
    "\n",
    "        for qrs_complex_counter in range(len(actual_qrs_complexes)):\n",
    "            if actual_qrs_complexes[qrs_complex_counter] < min_index:\n",
    "                min_counter += 1\n",
    "            if actual_qrs_complexes[qrs_complex_counter] < max_index:\n",
    "                max_counter += 1\n",
    "        split[split_signal_counter].append(actual_qrs_complexes[min_counter:max_counter]-min_index)\n",
    "\n",
    "    return split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create arrays to safe the detectors and databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = []\n",
    "databases = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Databases\n",
    "Create a `Database` object for `telehealth_environment_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "telehealth_path = join(current_working_directory,'databases/telehealth')\n",
    "telehealth_files = [f for f in listdir(telehealth_path) if isfile(join(telehealth_path, f))]\n",
    "\n",
    "telehealth_fs = 500\n",
    "users = []\n",
    "recordings = []\n",
    "for file in telehealth_files:\n",
    "    recordingsegments = []\n",
    "    data = pandas.read_csv(join(telehealth_path, file),sep=\",\",header=None)\n",
    "    signal = np.array(data[0]).astype(float)\n",
    "    qrs_complex_indices = np.array(data[1]).astype(int)\n",
    "    actual_qrs_complexes = []\n",
    "    \n",
    "    for indexcounter in range(len(qrs_complex_indices)):\n",
    "        if qrs_complex_indices[indexcounter]:\n",
    "            actual_qrs_complexes.append(indexcounter)\n",
    "    \n",
    "    splits = split_signal(signal=signal,fs=telehealth_fs, actual_qrs_complexes=np.array(actual_qrs_complexes).astype(int))\n",
    "    \n",
    "    for split in splits:\n",
    "        recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=telehealth_fs))\n",
    "    recordings.append(Recording(RecordingName=str(file),RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=telehealth_fs))\n",
    "users.append(User(UserName=\"default\",Recordings = recordings))   \n",
    "        \n",
    "telehealth_environment_database = Database(\n",
    "    Name=\"Telehealth Test Database\",\n",
    "    Users=users,\n",
    "    Fs=telehealth_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Database` object for `synth_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_fs = 500\n",
    "signal = np.transpose(np.array(returndata['multileadECG']))[:,0]\n",
    "actual_qrs_complexes = np.transpose(np.array(returndata['QRSindex']))[:,0].astype(int)\n",
    "\n",
    "splits = split_signal(signal=signal,fs=synth_fs,actual_qrs_complexes=actual_qrs_complexes)\n",
    "\n",
    "recordingsegments = []\n",
    "for split in splits:\n",
    "    recordingsegments.append(RecordingSegment(Signal=split[0],Actual_Qrs_Complex=split[1],Fs=synth_fs))\n",
    "recordings = [Recording(\"default\",RecordingSegments=recordingsegments, WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=synth_fs)]\n",
    "users = [User(UserName=\"default\",Recordings=recordings)]\n",
    "\n",
    "synth_database = Database(\n",
    "    Name=\"Synthetic data\", \n",
    "    Users=users,\n",
    "    Fs=synth_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Database` object for `wfdb_test_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfdb_fs = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0]).fs\n",
    "signal = np.array(wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=10000, channels=[0]).p_signal[:,0])\n",
    "actual_qrs_complexes = np.array(wfdb.rdann('sample-data/100','atr', sampfrom=0, sampto=10000).sample[1:]).astype(int)\n",
    "\n",
    "splits = split_signal(signal=signal, fs=wfdb_fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "\n",
    "recordingsegments = []\n",
    "for split in splits:\n",
    "    recordingsegments.append(RecordingSegment(Signal=split[0],Actual_Qrs_Complex=split[1],Fs=wfdb_fs))\n",
    "recordings = [Recording(\"default\",RecordingSegments=recordingsegments, WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=wfdb_fs)]\n",
    "users = [User(UserName=\"default\",Recordings=recordings)]\n",
    "\n",
    "wfdb_test_database = Database(\n",
    "    Name=\"WFDB Test Database\",\n",
    "    Users=users,\n",
    "    Fs=wfdb_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data from different databases to `databases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases.append(synth_database)\n",
    "#databases.append(wfdb_test_database)\n",
    "#databases.append(telehealth_environment_test_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Detectors\n",
    "Add detectors from different locations in the standaradized format to the detectors list.\n",
    "\n",
    "Add gqrs detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors.append(Detector(name=\"GQRS\", short_name=\"gqrs\", algorithm=processing.qrs.gqrs_detect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function and add jqrs detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "jqrs_algo_path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Playground\\detectors\\jqrs')\n",
    "eng.addpath(jqrs_algo_path, nargout=0)\n",
    "\n",
    "def run_jqrs_detector(signal, fs):\n",
    "    threshold = 0.6 # energy threshold of the detector in au, default = 0.6\n",
    "    ref_period = 0.250 # refractory period in sec between two R-peaks in ms, default = 0.250\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.qrs_detect2(matlab.double(newsignal), threshold, ref_period, matlab.double(fs)))[0].astype(int)\n",
    "\n",
    "detectors.append(Detector(name=\"JQRS\", short_name=\"jqrs\", algorithm=run_jqrs_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function and add visgraphdetector detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visgraph_detector(signal, fs):\n",
    "    beta = 0.55 # beta, default = 0.55\n",
    "    gamma = 0.5 # gamm, default = 0.5\n",
    "    lowcut = 4 # lowcut, default = 4\n",
    "    R_peaks, weights, weighted_signal = VisGraphDetector(fs).visgraphdetect(signal, beta=beta, gamma=gamma, lowcut=lowcut, M = 2*fs)\n",
    "    return R_peaks\n",
    "\n",
    "detectors.append(Detector(name=\"VisGraphDetector\", short_name=\"visgraph\", algorithm=run_visgraph_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function and add rpeakdetect detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpeakdetect_path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Playground\\detectors')\n",
    "eng.addpath(rpeakdetect_path, nargout=0)\n",
    "\n",
    "def run_rpeakdetect_detector(signal, fs):\n",
    "    threshhold = 0.2 # default = 0.2\n",
    "    testmode = 0 # default = 0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.rpeakdetect(matlab.double(newsignal), matlab.double(fs),threshhold,testmode)['R_index'])[0].astype(int)\n",
    "\n",
    "detectors.append(Detector(name=\"rpeakdetect\", short_name=\"rpeak\", algorithm=run_rpeakdetect_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function and add r_deco detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_deco_path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Playground\\detectors/r_deco')\n",
    "eng.addpath(r_deco_path, nargout=0)\n",
    "\n",
    "def run_r_deco_detector(signal, fs):\n",
    "    envelope_size = 300.0 # envelope size in ms, default = 300.0\n",
    "    average_heart_rate = 100.0 # average heart rate in bpm, default = 100.0\n",
    "    post_processing = 1.0 # post processing where 1.0 means yes, default = 1.0\n",
    "    ectopic_removal = 0.0 # ectopic removal where 1.0 means yes, default = 0.0\n",
    "    inverted_signal = 0.0 # inverted signal where 1.0 means yes, default = 0.0\n",
    "    parameters_check = 0.0 # parameters check in UI where 1.0 means yes, default = 0.0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.peak_detection([envelope_size,average_heart_rate,post_processing,ectopic_removal,inverted_signal],matlab.double(newsignal), matlab.double(fs),parameters_check)).astype(int)[0][0]\n",
    "\n",
    "detectors.append(Detector(name=\"r_deco\", short_name=\"rdeco\", algorithm=run_r_deco_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function and add unsw detector to the detectors list in the standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsw_path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Databases\\Telehealth')\n",
    "eng.addpath(unsw_path, nargout=0)\n",
    "\n",
    "def run_unsw_detector(signal, fs):\n",
    "    mask = [] # mask could be implemented later if wanted\n",
    "    plotting = 0 # 1.0 for ploting intermediate signals, 0.0 for no plotting, default = 0.0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.UNSW_QRSDetector(matlab.double(newsignal), matlab.double(fs),matlab.double(mask),matlab.double(plotting))).astype(int)[0]\n",
    "\n",
    "#detectors.append(Detector(name=\"UNSW_QRSDetector\", short_name=\"unsw\", algorithm=run_unsw_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the detectors from the `ecgdetectors` package into the right standard format to be able to iterate through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_average_detector(signal, fs):\n",
    "    return Detectors(fs).two_average_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_matched_filter_detector(signal, fs):\n",
    "        return Detectors(fs).matched_filter_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_swt_detector(signal, fs):\n",
    "    return Detectors(fs).swt_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_engzee_detector(signal, fs):\n",
    "    return Detectors(fs).engzee_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_christov_detector(signal, fs):\n",
    "    return Detectors(fs).christov_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_hamilton_detector(signal, fs):\n",
    "    return Detectors(fs).hamilton_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_pan_tompkins_detector(signal, fs):\n",
    "    return Detectors(fs).pan_tompkins_detector(unfiltered_ecg=signal)\n",
    "\n",
    "def run_wqrs_detector(signal, fs):\n",
    "    return Detectors(fs).wqrs_detector(unfiltered_ecg=signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add detectors from the `ecgdetectors` package to the detectors list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors.append(Detector(name=\"Elgendi et al (Two average)\", short_name=\"two_avg\", algorithm=run_two_average_detector))\n",
    "detectors.append(Detector(name=\"Matched filter\", short_name=\"match_fil\", algorithm=run_matched_filter_detector))\n",
    "detectors.append(Detector(name=\"Kalidas & Tamil (Wavelet transform)\", short_name=\"swt\", algorithm=run_swt_detector))\n",
    "detectors.append(Detector(name=\"Engzee\", short_name=\"engz\", algorithm=run_engzee_detector))\n",
    "detectors.append(Detector(name=\"Christov\", short_name=\"christ\", algorithm=run_christov_detector))\n",
    "detectors.append(Detector(name=\"Hamilton\", short_name=\"hamilt\", algorithm=run_hamilton_detector))\n",
    "detectors.append(Detector(name=\"Pan Tompkins\", short_name=\"pan_tomp\", algorithm=run_pan_tompkins_detector))\n",
    "detectors.append(Detector(name=\"WQRS\", short_name=\"wqrs\", algorithm=run_wqrs_detector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict QRS Complexes\n",
    "All algorithms are run and the resulting QRS complexes are saved in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "match_fil could not run because the sample rate is wrong and was skipped\n",
      "          gqrs  jqrs  visgraph  rpeak  rdeco  two_avg  match_fil   swt  engz  \\\n",
      "sensitiv  0.86   1.0      0.43   1.00    1.0     1.00        0.0  1.00   1.0   \n",
      "pos pred  1.00   1.0      0.60   0.88    1.0     0.78        0.0  0.78   1.0   \n",
      "f1 score  0.92   1.0      0.50   0.93    1.0     0.88        0.0  0.88   1.0   \n",
      "\n",
      "          christ  hamilt  pan_tomp  wqrs  \n",
      "sensitiv    1.00    1.00      1.00  1.00  \n",
      "pos pred    0.88    0.88      0.88  0.88  \n",
      "f1 score    0.93    0.93      0.93  0.93  \n"
     ]
    }
   ],
   "source": [
    "for database in databases:\n",
    "    for user in database.Users:\n",
    "        for recording in user.Recordings:\n",
    "            sensitivity = []\n",
    "            positive_predictivity = []\n",
    "            f1_score = []\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                evaluations = recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "                sensitivity_iter = [1 for evalunation in evaluations]\n",
    "                positive_predictivity_iter = [1 for evalunation in evaluations]\n",
    "                f1_score_iter = [1 for evalunation in evaluations]\n",
    "                for i in range(len(evaluations)):\n",
    "                    if evaluations[i].sensitivity < sensitivity_iter[i]: sensitivity_iter[i] = evaluations[i].sensitivity\n",
    "                    if evaluations[i].positive_predictivity < positive_predictivity_iter[i]: positive_predictivity_iter[i] = evaluations[i].positive_predictivity\n",
    "                    if evaluations[i].f1_score < f1_score_iter[i]: f1_score_iter[i] = evaluations[i].f1_score\n",
    "\n",
    "                sensitivity = sensitivity_iter\n",
    "                positive_predictivity = positive_predictivity_iter\n",
    "                f1_score = f1_score_iter\n",
    "\n",
    "            data = [np.around(sensitivity,decimals=2), np.around(positive_predictivity,decimals=2),np.around(f1_score,decimals=2)]\n",
    "            rows = [\"sensitiv\", \"pos pred\", \"f1 score\"]\n",
    "            columns = [detector.short_name for detector in detectors]\n",
    "\n",
    "            print(pandas.DataFrame(data, rows, columns))\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
