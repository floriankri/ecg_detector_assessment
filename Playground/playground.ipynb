{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG Beat Detector Comparison\n",
    "## Important Variables\n",
    "Important variables used throughout the code can be changed here.\n",
    "- `segmentation_window_size`: every signal in a database is sliced into pieces of `segmentation_window_size` seconds to make the signals more comparable\n",
    "- `tolerance_window_size`: to determine whether a QRS complex was determined correctly a tolerance window of Â±`tolerance_window_size` milliseconds is used\n",
    "- `print_detector_failure`: to decide whether a print should be shown if a detecor failes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_window_size = 30 # in seconds, default = 30\n",
    "tolerance_window_size = 150 # in milliseconds, default = 150\n",
    "print_detector_failure = 1 # print detector failure, 1 to print them, 0 to not print them, default = 0\n",
    "\n",
    "import_mit_bih_normal_sinus_rhythm_from_wfdb = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Start Engines\n",
    "Install all important libraries with pip apart from `matlab.engine` which has to be installed from the MATLAB root folder. You can find more information on instlaling the `matlab.engine` [here](https://de.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATLAB library\n",
    "import matlab.engine\n",
    "# general math libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib widget\n",
    "from scipy import stats\n",
    "# other general libraries\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from itertools import cycle\n",
    "import pandas\n",
    "import seaborn as sns \n",
    "# ecg signal and detector libraries\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "from ecgdetectors import Detectors\n",
    "from detectors.visgraphdetector import VisGraphDetector\n",
    "import neurokit2 as nk\n",
    "# setting the current working directory\n",
    "current_working_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the `matlab.engine` to be able to execute MATLAB commands or functions from within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classes and Functions\n",
    "`Detector` class to store all values necessary for a detector. This can later be added to a list of `detectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector():\n",
    "    def __init__(self, name, short_name, algorithm) -> None:\n",
    "        self.name = name\n",
    "        self.algorithm = algorithm\n",
    "        self.short_name = short_name\n",
    "    \n",
    "    def predicted_qrs_compelx(self, signal, fs):\n",
    "        return self.algorithm(signal, fs)\n",
    "\n",
    "    def name(self):\n",
    "        return self.name\n",
    "\n",
    "    def short_name(self):\n",
    "        return self.short_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `Evaluation` gets a `RecordingSegment` or `Recording` described below and the functions `predicted_peaks`, `binary_classification`, and `score_calculation`. The function `calculate` returns the whole element including `predicted_peaks`, number of predicted peaks `pp`, true positives `tp`, false positives `fp`, false negatives `fn`, `sensitivity`, `positive_predictivity`, and the `f1_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "    def __init__(self, RecordingSegment, Detector, predict_peaks, binary_classification, score_calculation) -> None:\n",
    "        self.RecordingSegment = RecordingSegment\n",
    "        self.Detector = Detector\n",
    "        self.predict_peaks = predict_peaks\n",
    "        self.binary_classification = binary_classification\n",
    "        self.score_calculation = score_calculation\n",
    "        self.predicted_peaks = None # predicted peaks as indexes\n",
    "        self.runtime = 0\n",
    "        self.pp = self.tp = self.fp = self.fn = None # number of predicted peaks, true positives, false positives, false negatives respectively\n",
    "        self.sensitivity = self.positive_predictivity = self.f1_score = None\n",
    "        self.failed = None\n",
    "    \n",
    "    def calculate(self):\n",
    "        self.predicted_peaks, self.runtime, self.failed = self.predict_peaks(self.RecordingSegment,self.Detector)\n",
    "        self.pp, self.tp, self.fp, self.fn = self.binary_classification(self.RecordingSegment, self.predicted_peaks)\n",
    "        self.sensitivity, self.positive_predictivity, self.f1_score = self.score_calculation(self.tp, self.fp, self.fn)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Database` class to store all values necessary for a databse. This can later be added to a list of `databases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database():\n",
    "    def __init__(self, Name, Users, Fs) -> None:\n",
    "        self.Name = Name\n",
    "        self.Users = Users\n",
    "        self.Fs = Fs\n",
    "\n",
    "    def Name(self):\n",
    "        return self.Name\n",
    "\n",
    "    def Users(self):\n",
    "        return self.Users\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Database` stores a list of `Users`. This `User` class stores important information about a user and a list to `Recordings` this user took. `User` can here also be used if a database stores arrythmia and sinus rythm signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User():\n",
    "    def __init__(self, UserName,Recordings) -> None:\n",
    "        self.UserName = UserName\n",
    "        self.Recordings = Recordings\n",
    "\n",
    "    def UserName(self):\n",
    "        return self.UserName\n",
    "\n",
    "    def Recordings(self):\n",
    "        return self.Recordings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `User` stores a list of `Recordings`. This `Recording` class stores important information about a recording and a list to `RecordingSegments` created from the signal in `Recording`. `Recording` stores the whole unsplit signal and list of actual QRS complexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recording():\n",
    "    def __init__(self, RecordingName, RecordingSegments, WholeSignal, WholeActual_Qrs_Complex, Fs, AdditionalInfo = None, Gender = None, Age = -1) -> None:\n",
    "        self.RecordingName = RecordingName\n",
    "        self.RecordingSegments = RecordingSegments\n",
    "        self.WholeSignal = WholeSignal\n",
    "        self.WholeActual_Qrs_Complex = WholeActual_Qrs_Complex\n",
    "        self.Fs = Fs\n",
    "        self.Evaluations = []\n",
    "        self.AdditionalInfo = AdditionalInfo\n",
    "        self.Gender = Gender\n",
    "        self.Age = Age\n",
    "\n",
    "    def RecordingName(self):\n",
    "        return self.RecordingName\n",
    "    \n",
    "    def RecordingSegments(self):\n",
    "        return self.RecordingSegments\n",
    "\n",
    "    def WholeSignal(self):\n",
    "        return self.WholeSignal\n",
    "    \n",
    "    def WholeActual_Qrs_Complex(self):\n",
    "        return self.WholeActual_Qrs_Complex\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs\n",
    "\n",
    "    def Evaluation(self, detectors, predict_peaks, binary_classification, score_calculation):\n",
    "        self.Evaluations = []\n",
    "        for detector in detectors:\n",
    "            self.Evaluations.append(Evaluation(self,detector,predict_peaks, binary_classification, score_calculation).calculate())\n",
    "        return self.Evaluations\n",
    "    \n",
    "    def addAdditionalInfo(self, AdditionalInfo):\n",
    "        self.AdditionalInfo = AdditionalInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Recording` stores a list of `RecordingSegments`. The `RecordingSegment` class stores important information about a segment of a recording. The function `Evaluation` which takes a list of `detectors` and three functions as an input returns a list of `Evaluation` objects, one for each detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordingSegment():\n",
    "    def __init__(self, Signal, Actual_Qrs_Complex, Fs, AdditionalInfo = None) -> None:\n",
    "        self.Signal = Signal\n",
    "        self.Actual_Qrs_Complex = Actual_Qrs_Complex\n",
    "        self.Fs = Fs\n",
    "        self.Evaluations = []\n",
    "        self.AdditionalInfo = AdditionalInfo\n",
    "\n",
    "    def Signal(self):\n",
    "        return self.Signal\n",
    "\n",
    "    def Actual_Qrs_Complex(self):\n",
    "        return self.Actual_Qrs_Complex\n",
    "\n",
    "    def Fs(self):\n",
    "        return self.Fs\n",
    "\n",
    "    def Evaluation(self, detectors, predict_peaks, binary_classification, score_calculation):\n",
    "        self.Evaluations = []\n",
    "        for detector in detectors:\n",
    "            self.Evaluations.append(Evaluation(self,detector,predict_peaks, binary_classification, score_calculation).calculate())\n",
    "        return self.Evaluations\n",
    "\n",
    "    def addAdditionalInfo(self, AdditionalInfo):\n",
    "        self.AdditionalInfo = AdditionalInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to split signals in smaller parts including the respecitve qrs complexes. `split_signal` returns an array of arrays where the inner arrays store the signals and the qrs complexes and the outer array stores the inner arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_signal(signal, fs, actual_qrs_complexes):\n",
    "    signal = np.array(signal)\n",
    "    actual_qrs_complexes = np.array(actual_qrs_complexes)\n",
    "    split = [[] for i in range(((len(signal)//(fs*segmentation_window_size)*2)-1))]\n",
    "\n",
    "    for split_signal_counter in range((len(signal)//(fs*segmentation_window_size)*2)-1):\n",
    "        min_index = split_signal_counter*(fs*segmentation_window_size//2)\n",
    "        max_index = split_signal_counter*(fs*segmentation_window_size//2)+fs*segmentation_window_size-1\n",
    "\n",
    "        split[split_signal_counter].append(signal[min_index:max_index])\n",
    "\n",
    "        min_counter = 0\n",
    "        max_counter = 0\n",
    "\n",
    "        for qrs_complex_counter in range(len(actual_qrs_complexes)):\n",
    "            if actual_qrs_complexes[qrs_complex_counter] < min_index:\n",
    "                min_counter += 1\n",
    "            if actual_qrs_complexes[qrs_complex_counter] < max_index:\n",
    "                max_counter += 1\n",
    "        split[split_signal_counter].append(actual_qrs_complexes[min_counter:max_counter]-min_index)\n",
    "\n",
    "    return split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Database Objects\n",
    "### Telehealth Database\n",
    "Create a `Database` object for `telehealth_environment_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = join(current_working_directory,'databases/telehealth')\n",
    "files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "fs = 500\n",
    "users = []\n",
    "recordings = []\n",
    "for file in files:\n",
    "    recordingsegments = []\n",
    "    data = pandas.read_csv(join(path, file),sep=\",\",header=None)\n",
    "    signal = np.array(data[0]).astype(float)\n",
    "    qrs_complex_indices = np.array(data[1]).astype(int)\n",
    "    actual_qrs_complexes = []\n",
    "    \n",
    "    for indexcounter in range(len(qrs_complex_indices)):\n",
    "        if qrs_complex_indices[indexcounter]:\n",
    "            actual_qrs_complexes.append(indexcounter)\n",
    "    \n",
    "    splits = split_signal(signal=signal,fs=fs, actual_qrs_complexes=np.array(actual_qrs_complexes).astype(int))\n",
    "    \n",
    "    for split in splits:\n",
    "        recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=fs))\n",
    "    if len(signal) >= segmentation_window_size*fs:\n",
    "        signal = signal[0:segmentation_window_size*fs]\n",
    "        actual_qrs_complexes = np.array([qrs for qrs in actual_qrs_complexes if qrs < (segmentation_window_size*fs-1)])\n",
    "    recordings.append(Recording(RecordingName=str(file),RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs))\n",
    "users.append(User(UserName=\"default\",Recordings = recordings))   \n",
    "        \n",
    "telehealth_environment_database = Database(\n",
    "    Name=\"Telehealth Test Database\",\n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Database\n",
    "Some initial parameters for the generation of the synthetic signals. Here it is possible to choose diffent types of artificial noise and also choose whether the signal should be real or synthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Initial parameters---\n",
    "rrLength = 50       # A desired ECG signal length (the number of RR intervals) \n",
    "APBrate = 0.10      # Rate of atrial premature beats (APB). A number between 0 and 0.5\n",
    "onlyRR = 0          # 1 - only RR intervals are generated, 0 - multilead ECG is generated\n",
    "\n",
    "medEpis = 15        # Median duration of an atrial fibrillation (AF) episode\n",
    "stayInAF = float(1-np.log(2)/medEpis)   # Probability to stay in AF state\n",
    "AFburden = 0.8      # AF burden. 0 - the entire signal is sinus rhythm (SR), 1 - the entire signal is AF\n",
    "\n",
    "noiseType = 4       # Type of noise. A number from 0 to 4. 0 - no noise added (noise RMS = 0 mV), \n",
    "                    # 1 - motion artefacts, 2 - electrode movement artefacts, 3 - baseline wander, \n",
    "                    # 4 - mixture of type 1, type 2 and type 3 noises\n",
    "noiseRMS = 0.02     # Noise level in milivolts \n",
    "\n",
    "realRRon = 1        # 1 - real RR series are used, 0 - synthetic\n",
    "realVAon = 1        # 1 - real ventricular activity is used, 0 - synthetic\n",
    "realAAon = 1        # 1 - real atrial activity is used, 0 - synthetic\n",
    "# Note: cannot select real atrial activity and synthetic ventricular activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a synthetic signal and safe it in `returndata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Signal_generator')\n",
    "eng.addpath(path, nargout=0)\n",
    "\n",
    "data = eng.simPAF_ECG_generator(rrLength, realRRon, realVAon, realAAon, AFburden, stayInAF, APBrate, noiseType, noiseRMS, onlyRR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Database` object for `synth_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 500\n",
    "signal = np.transpose(np.array(data['multileadECG']))[:,0]\n",
    "actual_qrs_complexes = np.transpose(np.array(data['QRSindex']))[:,0].astype(int)\n",
    "\n",
    "splits = split_signal(signal=signal,fs=fs,actual_qrs_complexes=actual_qrs_complexes)\n",
    "\n",
    "recordingsegments = []\n",
    "for split in splits:\n",
    "    recordingsegments.append(RecordingSegment(Signal=split[0],Actual_Qrs_Complex=split[1],Fs=fs))\n",
    "recordings = [Recording(\"default\",RecordingSegments=recordingsegments, WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs)]\n",
    "users = [User(UserName=\"default\",Recordings=recordings)]\n",
    "\n",
    "synth_database = Database(\n",
    "    Name=\"Synthetic data\", \n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WFDB Test Database\n",
    "Create a `Database` object for `wfdb_test_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=30000, channels=[0]).fs\n",
    "signal = np.array(wfdb.rdrecord('sample-data/100', sampfrom=0, sampto=30000, channels=[0]).p_signal[:,0])\n",
    "actual_qrs_complexes = np.array(wfdb.rdann('sample-data/100','atr', sampfrom=0, sampto=30000).sample[1:]).astype(int)\n",
    "\n",
    "splits = split_signal(signal=signal, fs=fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "\n",
    "recordingsegments = []\n",
    "for split in splits:\n",
    "    recordingsegments.append(RecordingSegment(Signal=split[0],Actual_Qrs_Complex=split[1],Fs=fs))\n",
    "recordings = [Recording(\"default\",RecordingSegments=recordingsegments, WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs)]\n",
    "users = [User(UserName=\"default\",Recordings=recordings)]\n",
    "\n",
    "wfdb_test_database = Database(\n",
    "    Name=\"WFDB Test Database\",\n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Arrhythmia Database\n",
    "Create a `Database` object for `mit_bih_arrhythmia_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\TestDatabases\\mit-bih-arrhythmia-database-1.0.0\"\n",
    "files = np.array(pandas.read_csv(join(path, \"RECORDS\"),header=None)[0])\n",
    "\n",
    "fs = wfdb.rdrecord(join(path,str(files[0])), channels=[0]).fs\n",
    "\n",
    "users = []\n",
    "\n",
    "for file in files:\n",
    "    recordings = []\n",
    "    recordingsegments = []\n",
    "    signal = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).p_signal[:,0]).astype(float)\n",
    "    actual_qrs_complexes = np.array(wfdb.rdann(join(path,str(file)),'atr').sample[1:]).astype(int)\n",
    "    comments = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).comments)\n",
    "    age = int(comments[0].split()[0])\n",
    "    gender = comments[0].split()[1]\n",
    "\n",
    "    splits = split_signal(signal=signal,fs=fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "    \n",
    "    for split in splits:\n",
    "        recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=fs))\n",
    "    recordings.append(Recording(RecordingName=\"default\",RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs, Gender=gender, Age=age, AdditionalInfo=comments))\n",
    "    users.append(User(UserName=str(file),Recordings = recordings))\n",
    "\n",
    "mit_bih_arrhythmia_database = Database(\n",
    "    Name=\"MIT-BIH Arrhythmia Database\",\n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Normal Sinus Rhythm Database\n",
    "Create a `Database` object for `mit_bih_normal_sinus_rhythm_database` including all sub objects necessary to initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_mit_bih_normal_sinus_rhythm_from_wfdb:\n",
    "    path = \"C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\TestDatabases\\mit-bih-normal-sinus-rhythm-database\"\n",
    "    files = np.array(pandas.read_csv(join(path, \"RECORDS\"),header=None)[0])\n",
    "\n",
    "    fs = wfdb.rdrecord(join(path,str(files[0])), channels=[0]).fs\n",
    "\n",
    "    users = []\n",
    "\n",
    "    for file in files:\n",
    "        recordings = []\n",
    "        recordingsegments = []\n",
    "        signal = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).p_signal[:,0]).astype(float)\n",
    "        actual_qrs_complexes = np.array(wfdb.rdann(join(path,str(file)),'atr').sample[1:]).astype(int)\n",
    "        comments = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).comments)\n",
    "        age = int(comments[0].split()[0])\n",
    "        gender = comments[0].split()[1]\n",
    "\n",
    "        splits = split_signal(signal=signal,fs=fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "        \n",
    "        for split in splits:\n",
    "            recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=fs))\n",
    "        recordings.append(Recording(RecordingName=\"default\",RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs, Gender=gender, Age=age, AdditionalInfo=comments))\n",
    "        users.append(User(UserName=str(file),Recordings = recordings))\n",
    "\n",
    "    mit_bih_normal_sinus_rhythm_database = Database(\n",
    "        Name=\"MIT-BIH Normal Sinus Rhythm Database\",\n",
    "        Users=users,\n",
    "        Fs=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_mit_bih_normal_sinus_rhythm_from_wfdb:\n",
    "    filename = 'pickle/mit_bih_normal_sinus_rhythm_database.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(mit_bih_normal_sinus_rhythm_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not import_mit_bih_normal_sinus_rhythm_from_wfdb:\n",
    "    filename = 'pickle/mit_bih_normal_sinus_rhythm_database.pkl'\n",
    "    file = open(filename, 'rb')\n",
    "    mit_bih_normal_sinus_rhythm_database = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Quality Database\n",
    "[Training](https://physionet.org/content/challenge-2014/1.0.0/set-p/#files-panel) part of the 2014 Physionet/CINC Challange data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\TestDatabases\\Physionet_CINC_Challange_2014/training_set\"\n",
    "files = np.array(pandas.read_csv(join(path, \"RECORDS\"),header=None)[0])\n",
    "\n",
    "fs = wfdb.rdrecord(join(path,str(files[0])), channels=[0]).fs\n",
    "\n",
    "users = []\n",
    "\n",
    "for file in files:\n",
    "    recordings = []\n",
    "    recordingsegments = []\n",
    "    signal = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).p_signal[:,0]).astype(float)\n",
    "    actual_qrs_complexes = np.array(wfdb.rdann(join(path,str(file)),'atr').sample[1:]).astype(int)\n",
    "    comments = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).comments)\n",
    "\n",
    "    splits = split_signal(signal=signal,fs=fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "    \n",
    "    for split in splits:\n",
    "        recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=fs))\n",
    "    recordings.append(Recording(RecordingName=\"default\",RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs, AdditionalInfo=comments))\n",
    "    users.append(User(UserName=str(file),Recordings = recordings))\n",
    "\n",
    "high_quality_database = Database(\n",
    "    Name=\"High Quality Database\",\n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Quality Database\n",
    "[Augmented Training](https://physionet.org/content/challenge-2014/1.0.0/set-p2/#files-panel) part of the 2014 Physionet/CINC Challange data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\TestDatabases\\Physionet_CINC_Challange_2014/augmented_training_set\"\n",
    "files = np.array(pandas.read_csv(join(path, \"RECORDS\"),header=None)[0])\n",
    "\n",
    "fs = wfdb.rdrecord(join(path,str(files[0])), channels=[0]).fs\n",
    "\n",
    "users = []\n",
    "\n",
    "for file in files:\n",
    "    recordings = []\n",
    "    recordingsegments = []\n",
    "    signal = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).p_signal[:,0]).astype(float)\n",
    "    actual_qrs_complexes = np.array(wfdb.rdann(join(path,str(file)),'atr').sample[1:]).astype(int)\n",
    "    comments = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).comments)\n",
    "\n",
    "    splits = split_signal(signal=signal,fs=fs, actual_qrs_complexes=actual_qrs_complexes)\n",
    "    \n",
    "    for split in splits:\n",
    "        recordingsegments.append(RecordingSegment(Signal=split[0], Actual_Qrs_Complex=split[1], Fs=fs))\n",
    "    recordings.append(Recording(RecordingName=\"default\",RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs, AdditionalInfo=comments))\n",
    "    users.append(User(UserName=str(file),Recordings = recordings))\n",
    "\n",
    "low_quality_database = Database(\n",
    "    Name=\"Low Quality Database\",\n",
    "    Users=users,\n",
    "    Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Detector Objects\n",
    "Create `Detector` objects for detectors from different locations in the standaradized format.\n",
    "\n",
    "Create `gqrs_detector` object. This detector stems from the wfdb toolbox. More information can be found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gqrs_detector = Detector(name=\"GQRS\", short_name=\"gqrs\", algorithm=processing.qrs.gqrs_detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `jqrs_detector` object. This detector is written in MATLAB and also executed via the MATLAB engine. Therefore a function has been written to get it into the right format. More information can be found in the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = eng.genpath(join(current_working_directory,'detectors\\jqrs'))\n",
    "eng.addpath(path, nargout=0)\n",
    "\n",
    "def run_jqrs_detector(signal, fs):\n",
    "    threshold = 0.6 # energy threshold of the detector in au, default = 0.6\n",
    "    ref_period = 0.250 # refractory period in sec between two R-peaks in ms, default = 0.250\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.qrs_detect2(matlab.double(newsignal), threshold, ref_period, matlab.double(fs)))[0].astype(int)\n",
    "\n",
    "jqrs_detector = Detector(name=\"JQRS\", short_name=\"jqrs\", algorithm=run_jqrs_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `visgraph_detector` object. For this detector a Python implementation is used. More information can be found in the respecitve file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_visgraph_detector(signal, fs):\n",
    "    beta = 0.55 # beta, default = 0.55\n",
    "    gamma = 0.5 # gamm, default = 0.5\n",
    "    lowcut = 4 # lowcut, default = 4\n",
    "    R_peaks, weights, weighted_signal = VisGraphDetector(fs).visgraphdetect(signal, beta=beta, gamma=gamma, lowcut=lowcut, M = 2*fs)\n",
    "    return R_peaks\n",
    "\n",
    "visgraph_detector = Detector(name=\"VisGraphDetector\", short_name=\"visgraph\", algorithm=run_visgraph_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `rpeakdetect_detector` object. This detector is written in MATLAB and also executed via the MATLAB engine. Therefore a function has been written to get it into the right format. More information can be found in the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = eng.genpath(join(current_working_directory,'detectors'))\n",
    "eng.addpath(path, nargout=0)\n",
    "\n",
    "def run_rpeakdetect_detector(signal, fs):\n",
    "    threshhold = 0.2 # default = 0.2\n",
    "    testmode = 0 # default = 0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.rpeakdetect(matlab.double(newsignal), matlab.double(fs),threshhold,testmode)['R_index'])[0].astype(int)\n",
    "\n",
    "rpeakdetect_detector = Detector(name=\"rpeakdetect\", short_name=\"rpeak\", algorithm=run_rpeakdetect_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `r_deco_detector` object. This detector is written in MATLAB and also executed via the MATLAB engine. Therefore a function has been written to get it into the right format. More information can be found in the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = eng.genpath(join(current_working_directory,'detectors/r_deco'))\n",
    "eng.addpath(path, nargout=0)\n",
    "\n",
    "def run_r_deco_detector(signal, fs):\n",
    "    envelope_size = 300.0 # envelope size in ms, default = 300.0\n",
    "    average_heart_rate = 100.0 # average heart rate in bpm, default = 100.0\n",
    "    post_processing = 1.0 # post processing where 1.0 means yes, default = 1.0\n",
    "    ectopic_removal = 0.0 # ectopic removal where 1.0 means yes, default = 0.0\n",
    "    inverted_signal = 0.0 # inverted signal where 1.0 means yes, default = 0.0\n",
    "    parameters_check = 0.0 # parameters check in UI where 1.0 means yes, default = 0.0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.peak_detection([envelope_size,average_heart_rate,post_processing,ectopic_removal,inverted_signal],matlab.double(newsignal), matlab.double(fs),parameters_check)).astype(int)[0][0]\n",
    "\n",
    "r_deco_detector = Detector(name=\"r_deco\", short_name=\"rdeco\", algorithm=run_r_deco_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `unsw_detector` object. This detector is written in MATLAB and also executed via the MATLAB engine. Therefore a function has been written to get it into the right format. More information can be found in the respective file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = eng.genpath(join(current_working_directory,'detectors/unsw'))\n",
    "eng.addpath(path, nargout=0)\n",
    "\n",
    "def run_unsw_detector(signal, fs):\n",
    "    mask = [] # mask could be implemented later if wanted\n",
    "    plotting = False # 1.0 for ploting intermediate signals, 0.0 for no plotting, default = 0.0\n",
    "    newsignal = [[i] for i in signal]\n",
    "    return np.array(eng.UNSW_QRSDetector(matlab.double(newsignal), matlab.double(fs),matlab.double(mask),bool(plotting))).astype(int)[0] #,matlab.double(mask),plotting)\n",
    "\n",
    "unsw_detector = Detector(name=\"UNSW_QRSDetector\", short_name=\"unsw\", algorithm=run_unsw_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the detectors from the `ecgdetectors` package into the right standard format. Additionally they are safed into their own objects. More information about the package can be found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_two_average_detector(signal, fs):\n",
    "    return Detectors(fs).two_average_detector(unfiltered_ecg=signal)\n",
    "\n",
    "two_average_detector = Detector(name=\"Elgendi et al (Two average)\", short_name=\"two_avg\", algorithm=run_two_average_detector)\n",
    "\n",
    "def run_matched_filter_detector(signal, fs):\n",
    "    return Detectors(fs).matched_filter_detector(unfiltered_ecg=signal)\n",
    "\n",
    "matched_filter_detector = Detector(name=\"Matched filter\", short_name=\"match_fil\", algorithm=run_matched_filter_detector)\n",
    "\n",
    "def run_swt_detector(signal, fs):\n",
    "    return Detectors(fs).swt_detector(unfiltered_ecg=signal)\n",
    "\n",
    "swt_detector = Detector(name=\"Kalidas & Tamil (Wavelet transform)\", short_name=\"swt\", algorithm=run_swt_detector)\n",
    "\n",
    "def run_engzee_detector(signal, fs):\n",
    "    return Detectors(fs).engzee_detector(unfiltered_ecg=signal)\n",
    "\n",
    "engzee_detector = Detector(name=\"Engzee\", short_name=\"engz\", algorithm=run_engzee_detector)\n",
    "\n",
    "def run_christov_detector(signal, fs):\n",
    "    return Detectors(fs).christov_detector(unfiltered_ecg=signal)\n",
    "\n",
    "christov_detector = Detector(name=\"Christov\", short_name=\"christ\", algorithm=run_christov_detector)\n",
    "\n",
    "def run_hamilton_detector(signal, fs):\n",
    "    return Detectors(fs).hamilton_detector(unfiltered_ecg=signal)\n",
    "\n",
    "hamilton_detector = Detector(name=\"Hamilton\", short_name=\"hamilt\", algorithm=run_hamilton_detector)\n",
    "\n",
    "def run_pan_tompkins_detector(signal, fs):\n",
    "    return Detectors(fs).pan_tompkins_detector(unfiltered_ecg=signal)\n",
    "\n",
    "pan_tompkins_detector = Detector(name=\"Pan Tompkins\", short_name=\"pan_tomp\", algorithm=run_pan_tompkins_detector)\n",
    "\n",
    "def run_wqrs_detector(signal, fs):\n",
    "    return Detectors(fs).wqrs_detector(unfiltered_ecg=signal)\n",
    "\n",
    "wqrs_detector = Detector(name=\"WQRS\", short_name=\"wqrs\", algorithm=run_wqrs_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to get the detectors from the `neurokit2` package into the right standard format. Additionally they are safed into their own objects. More information about the package can be found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_neurokit(signal, fs):\n",
    "    # neurokit (default)\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"neurokit\")\n",
    "    _, neurokit = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"neurokit\")\n",
    "    return neurokit[\"ECG_R_Peaks\"]\n",
    "\n",
    "neurokit_detector = Detector(name=\"neurokit\", short_name=\"nk\", algorithm=run_neurokit)\n",
    "\n",
    "def run_pantompkins1985(signal, fs):\n",
    "    # pantompkins1985\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"pantompkins1985\")\n",
    "    _, pantompkins1985 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"pantompkins1985\")\n",
    "    return pantompkins1985[\"ECG_R_Peaks\"]\n",
    "\n",
    "pantompkins1985_detector = Detector(name=\"pantompkins1985\", short_name=\"pan_tomp_nk\", algorithm=run_pantompkins1985)\n",
    "\n",
    "def run_nabian2018(signal, fs):\n",
    "    # nabian2018\n",
    "    _, nabian2018 = nk.ecg_peaks(signal, sampling_rate=fs, method=\"nabian2018\")\n",
    "    return nabian2018[\"ECG_R_Peaks\"]\n",
    "\n",
    "nabian2018_detector = Detector(name=\"nabian2018\", short_name=\"nab_nk\", algorithm=run_nabian2018)\n",
    "\n",
    "def run_hamilton2002(signal, fs):\n",
    "    # hamilton2002\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"hamilton2002\")\n",
    "    _, hamilton2002 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"hamilton2002\")\n",
    "    return hamilton2002[\"ECG_R_Peaks\"]\n",
    "\n",
    "hamilton2002_detector = Detector(name=\"hamilton2002\", short_name=\"ham_nk\", algorithm=run_hamilton2002)\n",
    "\n",
    "def run_martinez2003(signal, fs):\n",
    "    # martinez2003\n",
    "    _, martinez2003 = nk.ecg_peaks(signal, sampling_rate=fs, method=\"martinez2003\")\n",
    "    return martinez2003[\"ECG_R_Peaks\"]\n",
    "\n",
    "martinez2003_detector = Detector(name=\"martinez2003\", short_name=\"mart_nk\", algorithm=run_martinez2003)\n",
    "\n",
    "def run_zong2003(signal, fs):\n",
    "    # zong2003\n",
    "    _, zong2003 = nk.ecg_peaks(signal, sampling_rate=fs, method=\"zong2003\")\n",
    "    return zong2003[\"ECG_R_Peaks\"]\n",
    "\n",
    "zong2003_detector = Detector(name=\"zong2003\", short_name=\"zong_nk\", algorithm=run_zong2003)\n",
    "\n",
    "def run_christov2004(signal, fs):\n",
    "    # christov2004\n",
    "    _, christov2004 = nk.ecg_peaks(signal, sampling_rate=fs, method=\"christov2004\")\n",
    "    return christov2004[\"ECG_R_Peaks\"]\n",
    "\n",
    "christov2004_detector = Detector(name=\"christov2004\", short_name=\"chris_nk\", algorithm=run_christov2004)\n",
    "\n",
    "def run_gamboa2008(signal, fs):\n",
    "    # gamboa2008\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"gamboa2008\")\n",
    "    _, gamboa2008 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"gamboa2008\")\n",
    "    return gamboa2008[\"ECG_R_Peaks\"]\n",
    "\n",
    "gamboa2008_detector = Detector(name=\"gamboa2008\", short_name=\"gamb_nk\", algorithm=run_gamboa2008)\n",
    "\n",
    "def run_elgendi2010(signal, fs):\n",
    "    # elgendi2010\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"elgendi2010\")\n",
    "    _, elgendi2010 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"elgendi2010\")\n",
    "    return elgendi2010[\"ECG_R_Peaks\"]\n",
    "\n",
    "elgendi2010_detector = Detector(name=\"elgendi2010\", short_name=\"elg_nk\", algorithm=run_elgendi2010)\n",
    "\n",
    "def run_engzeemod2012(signal, fs):\n",
    "    # engzeemod2012\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"engzeemod2012\")\n",
    "    _, engzeemod2012 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"engzeemod2012\")\n",
    "    return engzeemod2012[\"ECG_R_Peaks\"]\n",
    "\n",
    "engzeemod2012_detector = Detector(name=\"engzeemod2012\", short_name=\"engz_nk\", algorithm=run_engzeemod2012)\n",
    "\n",
    "def run_kalidas2017(signal, fs):\n",
    "    # kalidas2017\n",
    "    cleaned = nk.ecg_clean(signal, sampling_rate=fs, method=\"kalidas2017\")\n",
    "    _, kalidas2017 = nk.ecg_peaks(cleaned, sampling_rate=fs, method=\"kalidas2017\")\n",
    "    return kalidas2017[\"ECG_R_Peaks\"]\n",
    "\n",
    "kalidas2017_detector = Detector(name=\"kalidas2017\", short_name=\"kali_nk\", algorithm=run_kalidas2017)\n",
    "\n",
    "def run_rodrigues2021(signal, fs):\n",
    "    # rodrigues2021\n",
    "    _, rodrigues2021 = nk.ecg_peaks(signal, sampling_rate=fs, method=\"rodrigues2021\")\n",
    "    return rodrigues2021[\"ECG_R_Peaks\"]\n",
    "\n",
    "rodrigues2021_detector = Detector(name=\"rodrigues2021\", short_name=\"rodr_nk\", algorithm=run_rodrigues2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation functions\n",
    "The function `predict_peaks` predicts peaks in a signal. It needs a segment which can either be a `RecordingSegment` or a `Recording` and a `Detector` as an input. The output is a vector of predicted peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_peaks(segment, detector):\n",
    "    failed = False\n",
    "\n",
    "    if isinstance(segment, RecordingSegment):\n",
    "        signal = segment.Signal\n",
    "        fs = segment.Fs\n",
    "\n",
    "    elif isinstance(segment, Recording):\n",
    "        signal = segment.WholeSignal\n",
    "        fs = segment.Fs\n",
    "\n",
    "    else:\n",
    "        raise Exception('You have to input either a RecordingSegment or a Recording as the segment.')\n",
    "        \n",
    "    \n",
    "    if detector.short_name == \"match_fil\" and (fs != 250 and fs != 360):\n",
    "        if print_detector_failure:\n",
    "            failed = True\n",
    "            print(detector.short_name, \"could not run because the sample rate is wrong and was skipped\")\n",
    "        return [], 0, failed\n",
    "\n",
    "    #predicted_peaks = detector.predicted_qrs_compelx(signal=signal, fs=fs)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        predicted_peaks = detector.predicted_qrs_compelx(signal=signal, fs=fs)\n",
    "    except IndexError:\n",
    "        if print_detector_failure:\n",
    "            failed = True\n",
    "            print(detector.short_name, \"failed due to an index error and was skipped\")\n",
    "        return [], 0, failed\n",
    "    except matlab.engine.MatlabExecutionError:\n",
    "        if print_detector_failure:\n",
    "            failed = True\n",
    "            print(detector.short_name, \"failed due to a MatlabExecutionError, most likely the signal quality was to bad\")\n",
    "        return [], 0, failed\n",
    "    runtime = time.time() - start\n",
    "    return predicted_peaks, runtime, failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `binary_classification` classifies whether the predicted peaks are true-positive, false-positive, and false-negative. It needs a segment which can either be a `RecordingSegment` or a `Recording` and a vector of `predicted_peaks` as an input. The output is the number of predicted peaks `pp`, the number of true-positive peaks `tp`, the number of false-positive peaks `fp`, and the number of false-negative peaks `fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification(segment, predicted_peaks):\n",
    "\n",
    "    if isinstance(segment, RecordingSegment):\n",
    "        actual_peaks = segment.Actual_Qrs_Complex\n",
    "\n",
    "    elif isinstance(segment, Recording):\n",
    "        actual_peaks = segment.WholeActual_Qrs_Complex\n",
    "\n",
    "    else:\n",
    "        raise Exception('You have to input either a RecordingSegment or a Recording as the segment.')\n",
    "\n",
    "    pp = len(predicted_peaks)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    actual_peaks_iter = actual_peaks\n",
    "\n",
    "    for predicted_peak in predicted_peaks:\n",
    "        tpdetect = 1\n",
    "        for i in range(len(actual_peaks_iter)):\n",
    "            if predicted_peak >= (actual_peaks_iter[i] - tolerance_window_size) and predicted_peak <= (actual_peaks_iter[i] + tolerance_window_size):\n",
    "                tp+=1\n",
    "                tpdetect = 0\n",
    "                actual_peaks_iter = np.delete(actual_peaks_iter, i)\n",
    "                break\n",
    "        if tpdetect:\n",
    "            fp+=1\n",
    "\n",
    "    for actualpeak in actual_peaks_iter:\n",
    "        fn+=1\n",
    "\n",
    "    return pp, tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calculation(tp, fp, fn):\n",
    "    try:\n",
    "        sensitivity = tp / (tp+fn)\n",
    "    except ZeroDivisionError:\n",
    "        sensitivity = 0\n",
    "    try:\n",
    "        positive_predictivity = tp / (tp+fp)\n",
    "    except ZeroDivisionError:\n",
    "        positive_predictivity = 0\n",
    "    try:    \n",
    "        f1_score = tp / (tp+.5*(fp+fn))\n",
    "    except ZeroDivisionError:\n",
    "        f1_score = 0\n",
    "    return sensitivity, positive_predictivity, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Databases and Detectors\n",
    "Create arrays to safe the `detectors` and `databases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = []\n",
    "detectors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data from different databases to `databases`. By commenting or uncommenting a database here, you can decide whether it should be used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databases.append(synth_database)\n",
    "databases.append(wfdb_test_database)\n",
    "#databases.append(telehealth_environment_database)\n",
    "#databases.append(mit_bih_arrhythmia_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add detectors to the `detectors`. By commenting or uncommenting a detector here, you can decide whether it should be used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detectors.append(gqrs_detector)\n",
    "detectors.append(jqrs_detector)\n",
    "# detectors.append(visgraph_detector) # takes a lot of time, not used because it takes long and does not give good results\n",
    "detectors.append(rpeakdetect_detector)\n",
    "detectors.append(r_deco_detector)\n",
    "detectors.append(unsw_detector)\n",
    "# detectors.append(matched_filter_detector) # in ecg-detectors package but not used in neurkit, might be to useless because of frequencies, not used because of frequency issue\n",
    "\n",
    "detectors.append(wqrs_detector) # differnt implementation of neurokit zong implementation, used because it works, rather slow though\n",
    "# detectors.append(zong2003_detector) # neurokit detector, different from wqrs, does not work, not used because implementation does not work\n",
    "\n",
    "# detectors.append(swt_detector) # same es neurokit kalidas implementation but a bit slower even though both refer to the ecg-detectors package, not used because neurokit implementation is faster\n",
    "detectors.append(kalidas2017_detector) # neurokit detector, same as swt, changed implementation slightly from ecg-detectors package\n",
    "\n",
    "detectors.append(two_average_detector) # same es neurokit elgendi implementation but a bit faster even though both refer to the ecg-detectors package\n",
    "# detectors.append(elgendi2010_detector) # neurokit detector, same as two_average, not used because base implementation is faster\n",
    "\n",
    "detectors.append(engzee_detector) # same es neurokit engzee implementation but a bit faster even though both refer to the ecg-detectors package\n",
    "# detectors.append(engzeemod2012_detector) # neurokit detector, same as engzee, not used because base implementation is faster\n",
    "\n",
    "detectors.append(christov_detector) # same es neurokit christov implementation but a bit faster even though both refer to the ecg-detectors package\n",
    "# detectors.append(christov2004_detector) # neurokit detector, not used because base implementation is faster\n",
    "\n",
    "detectors.append(hamilton_detector) # same es neurokit hamilton implementation but a bit faster even though both refer to the ecg-detectors package\n",
    "# detectors.append(hamilton2002_detector) # neurokit detector, same as hamilton, not used because base implementation is faster \n",
    "\n",
    "# detectors.append(pan_tompkins_detector) # same as neurokit pan_tompkins implementation but slower even though both refer to the ecg-detectors package, not used because neurokit implementation is faster\n",
    "detectors.append(pantompkins1985_detector) # neurokit detector, same as pan_tompkins\n",
    "\n",
    "detectors.append(neurokit_detector) # neurokit detector, new\n",
    "detectors.append(nabian2018_detector) # neurokit detector, new\n",
    "detectors.append(martinez2003_detector) # neurokit detector, new\n",
    "detectors.append(gamboa2008_detector) # neurokit detector, new\n",
    "# detectors.append(rodrigues2021_detector) # neurokit detector, fails often and does not give the best results if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict QRS Complexes\n",
    "All algorithms are run and the resulting QRS complexes are saved in vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for database in databases:\n",
    "    print('Database: ',database.Name)\n",
    "    runtime = [[] for i in range(len(detectors))]\n",
    "    sensitivity = [[] for i in range(len(detectors))]\n",
    "    positive_predictivity = [[] for i in range(len(detectors))]\n",
    "    f1_score = [[] for i in range(len(detectors))]\n",
    "    for user in database.Users:\n",
    "        print('User: ',user.UserName)\n",
    "        for recording in user.Recordings:\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                evaluations = recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "                for i in range(len(evaluations)):\n",
    "                    runtime[i].append(evaluations[i].runtime)\n",
    "                    sensitivity[i].append(evaluations[i].sensitivity)\n",
    "                    positive_predictivity[i].append(evaluations[i].positive_predictivity)\n",
    "                    f1_score[i].append(evaluations[i].f1_score)\n",
    "\n",
    "    data = [np.around([np.mean(time) for time in runtime],decimals=2),np.around([np.min(sens) for sens in sensitivity],decimals=2), np.around([np.min(pos_pred) for pos_pred in positive_predictivity],decimals=2),np.around([np.min(f1) for f1 in f1_score],decimals=2)]\n",
    "    rows = [\"runtime (s)\",\"sensitiv\", \"pos pred\", \"f1 score\"]\n",
    "    columns = [detector.short_name for detector in detectors]\n",
    "\n",
    "    print(pandas.DataFrame(data, rows, columns))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(range(1, len(runtime)+1),[np.mean(time) for time in runtime])\n",
    "    ax.set_ylabel(\"time (s)\")\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.boxplot(f1_score)\n",
    "    ax2.set_ylabel(\"f1_score (1)\")\n",
    "    ax.set_xticklabels(columns, rotation = 'vertical')\n",
    "    ax.set_title(database.Name) \n",
    "    ax.set_xlabel(\"detectors\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = databases[0].Name\n",
    "recording = databases[0].Users[0].Recordings[0]\n",
    "signal = recording.WholeSignal\n",
    "actual_qrs_complexes = recording.WholeActual_Qrs_Complex\n",
    "evaluations = recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"d\", \"v\", \"x\", \"+\", \"*\", \"^\", \"<\", \">\", \"1\", \"2\", \"3\", \"4\", \"8\", \"s\",\"p\", \"P\", \"h\", \"H\", \"X\", \"D\", \"|\", \"_\", \".\", \"o\"]\n",
    "colors = [\"r\", \"b\", \"c\", \"m\", \"y\", \"k\"]\n",
    "\n",
    "fmt = [marker + color for marker in markers for color in colors]\n",
    "\n",
    "#fmt = ['bo','r<','c^','m>','yv','rx','kd','cp','cx','b>','yx','mx','bx', 'rd', 'r0']\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(signal)\n",
    "ax.plot(actual_qrs_complexes, signal[actual_qrs_complexes], 'go', label = \"Actual QRS Complexes\")\n",
    "for i in range(len(evaluations)):\n",
    "    ax.plot(evaluations[i].predicted_peaks, signal[evaluations[i].predicted_peaks], fmt[i],label=evaluations[i].Detector.name)\n",
    "ax.set_title(name)\n",
    "ax.set_xlabel(\"time (steps)\")\n",
    "ax.set_ylabel(\"signal (mV)\")\n",
    "\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - Performance on synthetic data\n",
    "## Variable declaration\n",
    "Important variables are declared and changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataset = False # simulate synthetic dataset, True to simulate, False to skip simulation, default = Flase, simulation takes long\n",
    "evaluate_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal generator\n",
    "Synthetic signals are generated using the different parameters defined in the next cell. If `generate_dataset` is False the simulation is skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_dataset:\n",
    "    path = eng.genpath('C:/Users/flori\\OneDrive\\Dokumente\\TU\\Bachelor Thesis\\Code\\Signal_generator')\n",
    "    eng.addpath(path)\n",
    "\n",
    "\n",
    "    rrLength = [40]        # A desired ECG signal length (the number of RR intervals) \n",
    "    APBrate = [0]      # Rate of atrial premature beats (APB). A number between 0 and 0.5\n",
    "    onlyRR = [0]          # 1 - only RR intervals are generated, 0 - multilead ECG is generated\n",
    "\n",
    "    medEpis = [15]        # Median duration of an atrial fibrillation (AF) episode\n",
    "    stayInAF = 1-np.log(2)/medEpis   # Probability to stay in AF state\n",
    "    AFburden = [0, 1]      # AF burden. 0 - the entire signal is sinus rhythm (SR), 1 - the entire signal is AF\n",
    "\n",
    "    noiseType = [0, 1, 2, 3]       # Type of noise. A number from 0 to 4. 0 - no noise added (noise RMS = 0 mV), \n",
    "                                # 1 - motion artefacts, 2 - electrode movement artefacts, 3 - baseline wander, \n",
    "                                # 4 - mixture of type 1, type 2 and type 3 noises\n",
    "    noiseRMS = [0.0, 0.5]     # Noise level in milivolts \n",
    "\n",
    "    realRRon = [1]        # 1 - real RR series are used, 0 - synthetic\n",
    "    realVAon = [1]       # 1 - real ventricular activity is used, 0 - synthetic\n",
    "    realAAon = [1]        # 1 - real atrial activity is used, 0 - synthetic\n",
    "\n",
    "    repeats = 200\n",
    "\n",
    "    data = eng.Adapted_simPAF_ECG_generator_iterator(matlab.double(rrLength),matlab.double(APBrate),matlab.double(onlyRR),matlab.double(medEpis),matlab.double(stayInAF),matlab.double(AFburden),matlab.double(noiseType),matlab.double(noiseRMS),matlab.double(realRRon),matlab.double(realVAon),matlab.double(realAAon), matlab.double(repeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell calculates how many signals already existed before and saves this in `lastwritten`, to be able to add upon existing signals instead of creating the whole database from scratch every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_dataset:\n",
    "    path = join(current_working_directory,'databases/synth')\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    lastwritten = 0\n",
    "    lastwritten = int(files[-1][3:6])\n",
    "    print(lastwritten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All signals are saved to the harddrive in the **wfdb format**. This format was choosen as it is common for datasets and the resulting dataset can therefore be shared easily. All information created by the signal generator are saved as comments to make the dataset easily available. While only one lead is used for the experiments here, all of them are saved in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_dataset:\n",
    "    sig_name = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'X', 'Y', 'Z']\n",
    "    path = join(current_working_directory,'databases/synth')\n",
    "    fs = 500\n",
    "\n",
    "    for j in range(repeats):\n",
    "        for i  in range(len(data)//repeats):\n",
    "            synthetic_signal = data['s'+str(j+1)+'c'+str(i+1)]\n",
    "            signal = np.transpose(np.array(synthetic_signal['multileadECG']))[0:segmentation_window_size*fs]\n",
    "            actual_qrs_complexes = np.transpose(np.array(synthetic_signal['QRSindex'])).astype(int)\n",
    "            actual_qrs_complexes = np.array([qrs for qrs in actual_qrs_complexes if qrs < (segmentation_window_size*fs-1)])\n",
    "            \n",
    "            rrLength = synthetic_signal['ip_rrLength']\n",
    "            APBrate = synthetic_signal['ip_APBrate']\n",
    "            stayInAF = synthetic_signal['ip_stayInAF']\n",
    "            AFburden = synthetic_signal['ip_AFburden']\n",
    "            noiseType = synthetic_signal['ip_noiseType']\n",
    "            noiseRMS = synthetic_signal['ip_noiseRMS']\n",
    "            realRRon = synthetic_signal['ip_realRRon']\n",
    "            realVAon = synthetic_signal['ip_realVAon']\n",
    "            realAAon = synthetic_signal['ip_realAAon']\n",
    "\n",
    "            record_name = 'Rep'+str(format(j+lastwritten+1,'03d'))+'_AF' + str(int(AFburden)) + '_nT' + str(int(noiseType)) + '_nR' + str(format(noiseRMS, '.2f'))\n",
    "            record_name = record_name.replace('.','-')\n",
    "            record_comments = ['initial_rrLength='+str(rrLength),'APBrate='+str(APBrate),'stayInAF='+str(stayInAF),'AFburden='+str(AFburden),'noiseType='+str(noiseType),'noiseRMS='+str(noiseRMS),'realRRon='+str(realRRon),'realVAon='+str(realVAon),'realAAon='+str(realAAon),'repetition='+str(j+lastwritten+1),'element='+str(i+1),'signal_length(s)='+str(len(signal)//fs)]\n",
    "            wfdb.wrsamp(record_name=record_name, fs = fs, units=['mV' for k in range(len(signal[0,:]))], sig_name=sig_name, p_signal=signal, write_dir=path,comments=record_comments)\n",
    "            wfdb.wrann(record_name=record_name, fs= fs, extension='atr', sample=actual_qrs_complexes[:,0], symbol=['N' for l in range(len(actual_qrs_complexes))], write_dir=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate dataset\n",
    "Reads the dataset from wfdb format and saves it in an object. The difference to the pickled version below is that this one does not include the evaluations done below but only the initial signals ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_dataset:\n",
    "    path = join(current_working_directory, 'databases\\synth')\n",
    "    files = np.array([f[:-4] for f in listdir(path) if isfile(join(path, f)) and join(path, f).endswith('.dat')])\n",
    "\n",
    "    fs = wfdb.rdrecord(join(path,str(files[0])), channels=[0]).fs\n",
    "\n",
    "    repeats = int(files[-1][3:6])\n",
    "    options = len(files)//repeats\n",
    "\n",
    "    users = []\n",
    "\n",
    "    for option in range(options):\n",
    "        iterfiles = files[option::options]\n",
    "\n",
    "        recordings = []\n",
    "        for file in iterfiles:\n",
    "            \n",
    "            recordingsegments = []\n",
    "            signal = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).p_signal[:,0]).astype(float)\n",
    "            actual_qrs_complexes = np.array(wfdb.rdann(join(path,str(file)),'atr').sample[1:]).astype(int)\n",
    "\n",
    "            comments = np.array(wfdb.rdrecord(join(path,str(file)), channels=[0]).comments)\n",
    "            additionalinfo = {\n",
    "                'APBrate': float(comments[1][-3:]),\n",
    "                'stayInAF': float(comments[2].split('=')[1]),\n",
    "                'AFburden': float(comments[3].split('=')[1]),\n",
    "                'noiseType': float(comments[4].split('=')[1]),\n",
    "                'noiseRMS': float(comments[5].split('=')[1]),\n",
    "                'realRRon': float(comments[6].split('=')[1]),\n",
    "                'realVAon': float(comments[7].split('=')[1]),\n",
    "                'realAAon': float(comments[8].split('=')[1]),\n",
    "                'repetition': int(comments[9].split('=')[1]),\n",
    "                'element': int(comments[10].split('=')[1]),\n",
    "                'signal_length': int(comments[11].split('=')[1]),\n",
    "            }\n",
    "\n",
    "            recordings.append(Recording(RecordingName=str(file),RecordingSegments = recordingsegments,WholeSignal=signal,WholeActual_Qrs_Complex=actual_qrs_complexes,Fs=fs, AdditionalInfo=additionalinfo))\n",
    "        indexes = [3, 4, 5] \n",
    "        new_character = 'x'\n",
    "        for i in indexes:\n",
    "            name = iterfiles[0][:i] + new_character + iterfiles[0][i+1:]\n",
    "        users.append(User(UserName=name,Recordings = recordings))\n",
    "\n",
    "    synth_whole_database = Database(\n",
    "        Name=\"Whole Synthetic Database\",\n",
    "        Users=users,\n",
    "        Fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_dataset:\n",
    "    print('Database: ',synth_whole_database.Name)\n",
    "    users = synth_whole_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            #print('Recording: ',recording.RecordingName)\n",
    "            recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_dataset:\n",
    "    filename = 'pickle/synth_whole_database_evaluated.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(synth_whole_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "Reads a previously evaluated dataset to perform analysis on it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "#from IPython.display import set_matplotlib_formats\n",
    "#set_matplotlib_formats('svg')\n",
    "\n",
    "# # in newer versions of IPython, use \n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "from tol_colors import tol_cmap, tol_cset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pickle/synth_whole_database_evaluated.pkl'\n",
    "file = open(filename, 'rb')\n",
    "synth_whole_database = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signaltonoiseScipy(a, axis=0, ddof=0):\n",
    "    a = np.asanyarray(a)\n",
    "    m = a.mean(axis)\n",
    "    sd = a.std(axis=axis, ddof=ddof)\n",
    "    return 20*np.log10(np.abs(np.where(sd == 0, 0, m/sd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the basic evaluations in a graphical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = [Evaluation.Detector for Evaluation in synth_whole_database.Users[0].Recordings[0].Evaluations]\n",
    "detector_names = [detector.short_name for detector in detectors]\n",
    "fs = synth_whole_database.Users[0].Recordings[0].Fs\n",
    "users_all = synth_whole_database.Users\n",
    "\n",
    "print('Database: ',synth_whole_database.Name)\n",
    "\n",
    "users = []\n",
    "user_names = []\n",
    "for user in users_all:\n",
    "    record_name = user.UserName.replace('-','.')\n",
    "    noiseRMS = float(record_name[-4:])\n",
    "    noiseType = int(record_name[-8])\n",
    "    if (not noiseRMS and noiseType) or (noiseRMS and not noiseType):\n",
    "        continue\n",
    "    users.append(user)\n",
    "    user_names.append(user.UserName)\n",
    "\n",
    "runtime = [[[] for user in range(len(users))] for detector in range(len(detectors))]\n",
    "sensitivity = [[[] for user in range(len(users))] for detector in range(len(detectors))]\n",
    "positive_predictivity = [[[] for user in range(len(users))] for detector in range(len(detectors))]\n",
    "f1_score = [[[] for user in range(len(users))] for detector in range(len(detectors))]\n",
    "failed = [0 for detector in range(len(detectors))]\n",
    "SNR_ms = [[] for user in range(len(users))]\n",
    "SNR_dB = [[] for user in range(len(users))]\n",
    "SNR_ML = [[] for user in range(len(users))]\n",
    "\n",
    "data_f1_score = []\n",
    "data_detector = []\n",
    "data_user = []\n",
    "data_af = []\n",
    "data_noiseType = []\n",
    "data_failed = []\n",
    "data_runtime = []\n",
    "\n",
    "for j in range(len(users)):\n",
    "    \n",
    "    for recording in users[j].Recordings:\n",
    "        SNR_ms[j].append(signaltonoiseScipy(recording.WholeSignal))#np.mean(recording.WholeSignal)/(np.std(recording.WholeSignal)))\n",
    "        SNR_dB[j].append(20*np.log10((np.max(recording.WholeSignal)-np.min(recording.WholeSignal))/(2*recording.AdditionalInfo[\"noiseRMS\"] if recording.AdditionalInfo[\"noiseRMS\"] != 0 else 0.001)))\n",
    "        SNR_ML[j].append(eng.snr(matlab.double(recording.WholeSignal)))#,fs))\n",
    "        evaluations = recording.Evaluations\n",
    "        for i in range(len(evaluations)):\n",
    "            runtime[i][j].append(evaluations[i].runtime)\n",
    "            sensitivity[i][j].append(evaluations[i].sensitivity)\n",
    "            positive_predictivity[i][j].append(evaluations[i].positive_predictivity)\n",
    "            f1_score[i][j].append(evaluations[i].f1_score)\n",
    "            if evaluations[i].failed:\n",
    "                failed[i] = failed[i] + 1\n",
    "            \n",
    "            # creating arrays for a data frame\n",
    "            data_f1_score.append(evaluations[i].f1_score)\n",
    "            data_detector.append(evaluations[i].Detector.short_name)\n",
    "            data_user.append(users[j].UserName)\n",
    "            if int(users[j].UserName[9]):\n",
    "                data_af.append('af')\n",
    "            else:\n",
    "                data_af.append('sr')\n",
    "            data_noiseType.append(['no noise','motion artefacts','electrode movement','baseline wander'][int(users[j].UserName[13])])\n",
    "            data_failed.append(evaluations[i].failed)\n",
    "            data_runtime.append(evaluations[i].runtime)\n",
    "\n",
    "data = [np.around([np.mean(np.mean(time)) for time in runtime],decimals=2),np.around([np.min(np.min(sens)) for sens in sensitivity],decimals=2), np.around([np.min(np.min(pos_pred)) for pos_pred in positive_predictivity],decimals=2),np.around([np.min(np.min(f1)) for f1 in f1_score],decimals=2)]\n",
    "rows = [\"runtime (s)\",\"sensitiv\", \"pos pred\", \"f1 score\"]\n",
    "columns = [detector.short_name for detector in detectors]\n",
    "\n",
    "print(pandas.DataFrame(data, rows, columns))\n",
    "\n",
    "data = pandas.DataFrame(data=np.transpose(np.array([data_user,data_detector,data_f1_score,data_af,data_noiseType,data_failed,data_runtime])),columns=['user','detector','f1_score','af_sr','noiseType','failed','runtime'])\n",
    "data['f1_score'] = data['f1_score'].astype(float)\n",
    "\n",
    "f1_score_print = [[] for f1 in f1_score]\n",
    "for i in range(len(f1_score)):\n",
    "    for j in range(len(f1_score[i])):\n",
    "        for f1 in f1_score[i][j]:\n",
    "            f1_score_print[i].append(f1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(range(1, len(runtime)+1),[np.mean(np.mean(time)) for time in runtime])\n",
    "ax.set_ylabel(\"time (s)\")\n",
    "ax.set_title(synth_whole_database.Name) \n",
    "ax.set_xlabel(\"detectors\")\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.boxplot(f1_score_print)\n",
    "ax2.set_ylabel(\"f1_score (1)\")\n",
    "\n",
    "ax3 = ax.twinx()\n",
    "ax3.plot(range(1, len(detectors)+1),np.array(failed)*len(detectors)/np.array(f1_score).size*100, \"r-\")\n",
    "ax3.set_ylabel(\"failures (%)\")\n",
    "ax3.spines.right.set_position((\"axes\", 1.2))\n",
    "\n",
    "\n",
    "ax.set_xticks(range(1, len(detectors)+1))\n",
    "ax.set_xticklabels(columns, rotation = 'vertical')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = tol_cset('light')\n",
    "cmap = matplotlib.colors.to_rgba_array(cmap)\n",
    "fig, ax = plt.subplots()\n",
    "flierprops = dict(marker='o', markersize=0.5)\n",
    "sns.boxplot(x='detector', y='f1_score', hue='af_sr',data=data,palette=cmap,saturation=1,linewidth=0.5,flierprops=flierprops)\n",
    "\n",
    "ax.set_xticklabels(detector_names, rotation = 'vertical')\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for detector_name in detector_names:\n",
    "    x = data[(data['af_sr'] == 'af') & (data['noiseType'] == 'no noise') & (data['detector'] == detector_name)]['f1_score']\n",
    "    y = data[(data['af_sr'] == 'sr') & (data['noiseType'] == 'no noise') & (data['detector'] == detector_name)]['f1_score']\n",
    "    ranksums = stats.mannwhitneyu(x=x, y=y)\n",
    "    if ranksums.pvalue < 0.05: \n",
    "        print(ranksums)\n",
    "    else:\n",
    "        print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_number = data.copy(deep=True)\n",
    "for t in ['user','detector','af_sr','noiseType','failed','runtime']:\n",
    "    data_as_number[t] = data[t].astype('category').cat.codes\n",
    "\n",
    "corr = data_as_number.corr(method='kendall')\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(corr, \n",
    "           xticklabels=corr.columns.values, \n",
    "           yticklabels=corr.columns.values, \n",
    "           cmap=\"YlGnBu\",\n",
    "            annot=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.twinx()\n",
    "#ax3 = ax.twinx()\n",
    "ax.boxplot(SNR_ms,vert=True, patch_artist=True, positions=range(1, 2*len(user_names)+1,2))\n",
    "ax2.boxplot(SNR_dB, positions=range(2, 2*len(user_names)+1,2))\n",
    "#ax3.boxplot(SNR_ML, positions=range(2, 2*len(user_names)+1,2))\n",
    "ax.set_xticks(range(1, 2*len(user_names)+1,2))\n",
    "ax.set_xticklabels(user_names, rotation = 'vertical')\n",
    "ax.legend([\"mean/std\",\"20*log() in dB\"])\n",
    "ax.xaxis.grid(True)\n",
    "ax.yaxis.grid(True)\n",
    "ax.set_xlabel(\"sample groups\")\n",
    "ax.set_ylabel(\"signal to noise ratio (1)\")\n",
    "#ax2.set_ylabel(\"signal to noise ratio (db)\")\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Performance on three databases\n",
    "## Variable declaration\n",
    "Important variables are declared and changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_telehealth = False\n",
    "evaluate_mit_bih_arrhythmia = False\n",
    "evaluate_mit_bih_sinus_rhythm = False\n",
    "evaluate_high_quality = False\n",
    "evaluate_low_quality = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telehealth Database\n",
    "The database is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_telehealth:\n",
    "    print('Database: ',telehealth_environment_database.Name)\n",
    "    users = telehealth_environment_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_telehealth:\n",
    "    filename = 'pickle/telehealth_environment_database_evaluated.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(telehealth_environment_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT BIH Arrhythmia Database\n",
    "The database is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_mit_bih_arrhythmia:\n",
    "    print('Database: ',mit_bih_arrhythmia_database.Name)\n",
    "    users = mit_bih_arrhythmia_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            #recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_mit_bih_arrhythmia:\n",
    "    filename = 'pickle/mit_bih_arrhythmia_database_evaluated.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(mit_bih_arrhythmia_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Normal Sinus Rhythm Database\n",
    "The database is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_mit_bih_sinus_rhythm:\n",
    "    print('Database: ',mit_bih_normal_sinus_rhythm_database.Name)\n",
    "    users = mit_bih_normal_sinus_rhythm_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            #recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_mit_bih_sinus_rhythm:\n",
    "    filename = 'pickle/mit_bih_normal_sinus_rhythm_database.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(mit_bih_normal_sinus_rhythm_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Quality Database\n",
    "The database is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_high_quality:\n",
    "    print('Database: ',high_quality_database.Name)\n",
    "    users = high_quality_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_high_quality:\n",
    "    filename = 'pickle/high_quality_database_evaluated.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(high_quality_database, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Quality Database\n",
    "The database is evaluated. All algorithms are run for every signal and for each of them different metrics are created. This step takes quite long. It can be skipped by loading a previus version of the evaluated dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_low_quality:\n",
    "    print('Database: ',low_quality_database.Name)\n",
    "    users = low_quality_database.Users\n",
    "    for j in range(len(users)):\n",
    "        print('User: ',users[j].UserName)\n",
    "        for recording in users[j].Recordings:\n",
    "            recording.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)\n",
    "            print('Recording: ',recording.RecordingName)\n",
    "            for recordingsegment in recording.RecordingSegments:\n",
    "                recordingsegment.Evaluation(detectors, predict_peaks, binary_classification, score_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle evaluated dataset for easy use afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluate_low_quality:\n",
    "    filename = 'pickle/low_quality_database_evaluated.pkl'\n",
    "    file = open(filename, 'wb')\n",
    "    pickle.dump(low_quality_database, file)\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
